# -*- coding: utf-8 -*-
"""HW1Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CeGHAdV-C8QUVbu_-dBcb48jEWlpcbVP
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

import numpy as np
np.random.seed(42)

x1s = 2 * np.random.rand(100, 1)
x2s = 2 * np.random.rand(100,1)

bias = 8
w1 = 5
w2 = 3
ys = (w1 * x1s) + (w2 * x2s) + bias + np.random.rand(100, 1)

from mpl_toolkits import mplot3d
fig = plt.figure(figsize=(10, 8))
ax = plt.axes(projection='3d')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')
scatterplot = ax.scatter3D(x1s, x2s, ys)

# split the data into training and test sets
# train set
train_x1s = x1s[:80]
train_x2s = x2s[:80]
train_ys = ys[:80]
# test set
test_x1s = x1s[80:]
test_x2s = x2s[80:]
test_ys = ys[80:]

"""# Linear regression using numpy"""

# number of epochs
epochs = 10
# learning rate
lr = 0.01

# initial value for weight w and bias b
w1 = np.random.randn(1)
w2 = np.random.randn(1)
b = np.zeros(1)

for epoch in np.arange(epochs):
  for i in np.arange(80):
    y_pred = w1 * train_x1s[i] + w2 * train_x2s[i] + b
    
    grad_w1 = (y_pred - train_ys[i]) * train_x1s[i]
    grad_w2 = (y_pred - train_ys[i]) * train_x2s[i]
    grad_b = (y_pred - train_ys[i])
    
    w1 -= lr * grad_w1
    w2 -= lr * grad_w2
    b -= lr * grad_b

test_loss = 0
for i in np.arange(20):
  test_loss += 0.5 * (w1 * test_x1s[i] + w2 * test_x2s[i] + b - test_ys[i]) ** 2
test_loss /= 20

test_loss

pred_ys = w1 * test_x1s + w2 * test_x2s + b

fig = plt.figure(figsize=(12, 12))
ax = plt.axes(projection='3d')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')
scatterplot = ax.scatter3D(test_x1s, test_x2s, test_ys, color="blue")
scatterplot = ax.scatter3D(test_x1s, test_x2s, pred_ys, color="green")

b

w1

w2